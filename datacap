import cv2
import mediapipe as mp
import numpy as np
import os
import time
from datetime import datetime

# --- Setup ---
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils

cap = cv2.VideoCapture(0)

OUTPUT_DIR = r"C:\Users\Andrew\Desktop\PALAD\roitestdata2"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# --- Sequential file counter (5-digit filenames like 00001.jpg) ---
def get_next_index():
    existing = [f for f in os.listdir(OUTPUT_DIR) if f.lower().endswith(".jpg")]
    nums = []
    for f in existing:
        name = os.path.splitext(f)[0]
        if name.isdigit():
            try:
                nums.append(int(name))
            except:
                pass
    return max(nums) + 1 if nums else 1

global_file_index = get_next_index()

stable_counter = 0
REQUIRED_STABLE_FRAMES = 5
prev_center = None

def variance_of_laplacian(image):
    return cv2.Laplacian(image, cv2.CV_64F).var()

def to_xy(lm, w, h):
    # Accept either NormalizedLandmark or np.array([x,y])
    if hasattr(lm, 'x') and hasattr(lm, 'y'):
        return np.array([int(lm.x * w), int(lm.y * h)], dtype=np.int32)
    else:
        # assume lm is np.array([x, y])
        return np.array([int(lm[0] * w), int(lm[1] * h)], dtype=np.int32)


with mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=1,
    min_detection_confidence=0.7,
    min_tracking_confidence=0.7
) as hands:

    while cap.isOpened():
        success, frame = cap.read()
        if not success:
            continue

        # --- 1. Raw Input Preprocessing (kept minimal: flip + rgb conversion for mediapipe) ---
        frame = cv2.flip(frame, 1)
        clean_frame = frame.copy()
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        h, w, _ = frame.shape

        result = hands.process(rgb)

        roi_final = None  # will hold the raw cropped (rotation-normalized) color ROI when gated
        prompt = "Place your hand in the frame."
        distance_ok = False
        palm_ok = False
        open_ok = False
        sharp_ok = False

        if result.multi_hand_landmarks:
            hand_landmarks = result.multi_hand_landmarks[0]
            hand_label = result.multi_handedness[0].classification[0].label

            # --- 2. Landmark Extraction ---
            wrist = np.array([hand_landmarks.landmark[0].x, hand_landmarks.landmark[0].y])
            index_base = np.array([hand_landmarks.landmark[5].x, hand_landmarks.landmark[5].y])
            pinky_base = np.array([hand_landmarks.landmark[17].x, hand_landmarks.landmark[17].y])
            fingertips = [hand_landmarks.landmark[i] for i in [4, 8, 12, 16, 20]]

            # --- 3. Palm vs Back Classification ---
            v1 = index_base - wrist
            v2 = pinky_base - wrist
            normal = np.cross(np.append(v1, 0), np.append(v2, 0))
            normal /= np.linalg.norm(normal) + 1e-6
            if hand_label == "Left":
                normal = -normal

            if normal[2] <= 0:
                prompt = "Palm facing camera."
                palm_ok = False
            else:
                palm_ok = True

            if not palm_ok:
                cv2.putText(frame, prompt, (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)
                # single waitKey here to handle ESC
                if cv2.waitKey(5) & 0xFF == 27:
                    break
                cv2.imshow("Camera Feed", frame)
                continue

            # --- Convert to pixels ---
            wrist_xy = to_xy(wrist, w, h)
            index_base_xy = to_xy(index_base, w, h)
            pinky_base_xy = to_xy(pinky_base, w, h)
            fingertip_xy = [to_xy(f, w, h) for f in fingertips]

            # --- 4. Distance Estimation ---
            base_vec = pinky_base_xy - index_base_xy
            base_width_px = np.linalg.norm(base_vec)
            if base_width_px > 280:
                prompt = "Move hand farther away."
            elif base_width_px < 140:
                prompt = "Move hand closer."
            else:
                distance_ok = True

            if not distance_ok:
                cv2.putText(frame, prompt, (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)
                if cv2.waitKey(5) & 0xFF == 27:
                    break
                cv2.imshow("Camera Feed", frame)
                continue

            # --- 5. Palm Openness Check ---
            fingertip_distances = [np.linalg.norm(ft - wrist_xy) for ft in fingertip_xy]
            mean_finger_dist = np.mean(fingertip_distances)
            openness_ratio = mean_finger_dist / base_width_px

            if openness_ratio < 0.7:
                prompt = "Open your palm fully."
            else:
                open_ok = True

            if not open_ok:
                cv2.putText(frame, prompt, (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)
                if cv2.waitKey(5) & 0xFF == 27:
                    break
                cv2.imshow("Camera Feed", frame)
                continue

            # --- 6. ROI Geometry + Rotation Normalization ---
            mid_top = (index_base_xy + pinky_base_xy) / 2
            down_vec = wrist_xy - mid_top
            perp_vec = down_vec - (np.dot(down_vec, base_vec)/np.dot(base_vec, base_vec)) * base_vec
            perp_unit = perp_vec / (np.linalg.norm(perp_vec) + 1e-6)
            base_unit = base_vec / (np.linalg.norm(base_vec) + 1e-6)
            roi_size = int(np.linalg.norm(base_vec))
            if np.dot(wrist_xy - mid_top, perp_unit) < 0:
                perp_unit = -perp_unit

            top_left = index_base_xy
            top_right = pinky_base_xy
            bottom_right = (pinky_base_xy + perp_unit * roi_size).astype(int)
            bottom_left = (index_base_xy + perp_unit * roi_size).astype(int)
            pts = np.array([top_left, top_right, bottom_right, bottom_left], np.int32).reshape((-1,1,2))

            # --- Finger-overlap check ---
            finger_overlaps = False
            roi_poly = pts.reshape((-1, 2))

            for ft in fingertip_xy:
                if cv2.pointPolygonTest(roi_poly.astype(np.float32), (float(ft[0]), float(ft[1])), False) >= 0:
                    finger_overlaps = True
                    break

            if finger_overlaps:
                prompt = "Remove fingers from the ROI."
                stable_counter = 0  # prevent capture
                cv2.putText(frame, prompt, (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)
                if cv2.waitKey(5) & 0xFF == 27:
                    break
                cv2.imshow("Camera Feed", frame)
                continue

            # --- Directional guidance ---
            roi_center = np.mean(pts[:,0,:], axis=0)

            # --- Motion detection ---
            global_move_ok = True
            if prev_center is not None:
                movement = np.linalg.norm(roi_center - prev_center)
                if movement > 8:
                    global_move_ok = False
                    stable_counter = 0
                    prompt = "Keep your hand still."

            prev_center = roi_center.copy()

            if not global_move_ok:
                cv2.putText(frame, prompt, (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)
                if cv2.waitKey(5) & 0xFF == 27:
                    break
                cv2.imshow("Camera Feed", frame)
                continue

            offset = roi_center - np.array([w/2, h/2])
            threshold = 40
            if offset[0] > threshold:
                prompt = "Move your hand to the left."
            elif offset[0] < -threshold:
                prompt = "Move your hand to the right."
            elif offset[1] > threshold:
                prompt = "Move your hand upward."
            elif offset[1] < -threshold:
                prompt = "Move your hand downward."
            else:
                prompt = "Palm roughly centered."

            # --- Rotation ---
            angle = np.degrees(np.arctan2(base_unit[1], base_unit[0]))
            center = (int(mid_top[0]), int(mid_top[1]))
            rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)
            if hand_label == "Left":
                rotation_matrix = cv2.getRotationMatrix2D(center, angle + 180, 1.0)

            rotated = cv2.warpAffine(clean_frame, rotation_matrix, (w, h))
            pts_rotated = cv2.transform(pts, rotation_matrix)

            x_min, y_min = np.min(pts_rotated[:,0,:], axis=0)
            x_max, y_max = np.max(pts_rotated[:,0,:], axis=0)
            x_min, y_min = np.clip([x_min, y_min], 0, [w-1, h-1])
            x_max, y_max = np.clip([x_max, y_max], 0, [w-1, h-1])
            if x_max <= x_min or y_max <= y_min:
                continue

            roi_candidate = rotated[int(y_min):int(y_max), int(x_min):int(x_max)]

            # --- 8. ROI Validation + Sharpness (only checking sharpness; NO preprocessing) ---
            if roi_candidate.shape[0] < 100 or roi_candidate.shape[1] < 100:
                prompt = "Move closer."
                sharp_ok = False
            else:
                # compute sharpness on a resized gray copy, but do NOT use that image for saving
                gray_for_sharp = cv2.cvtColor(cv2.resize(roi_candidate, (256,256)), cv2.COLOR_BGR2GRAY)
                sharpness = variance_of_laplacian(gray_for_sharp)
                print(f"[DEBUG] ROI size: {roi_candidate.shape}, Sharpness: {sharpness:.2f}")
                if sharpness < 15:
                    prompt = "Palm too blurry."
                    sharp_ok = True  # allow borderline
                else:
                    sharp_ok = True

            # --- 9. Stability gating ---
            if all([palm_ok, distance_ok, open_ok, sharp_ok]):
                stable_counter += 1
                prompt = f"Hold still... scanning ({stable_counter}/{REQUIRED_STABLE_FRAMES})"
            else:
                stable_counter = 0

            if stable_counter >= REQUIRED_STABLE_FRAMES:
                # *** IMPORTANT CHANGE: do NOT perform preprocessing or save automatically ***
                # Set roi_final to the raw color ROI_candidate (rotation-normalized crop).
                roi_final = roi_candidate.copy()
                # Optionally flip for left hand to keep consistent orientation (kept minimal)
                if hand_label == "Left":
                    roi_final = cv2.flip(roi_final, 1)

                stable_counter = REQUIRED_STABLE_FRAMES  # keep gated state
                prompt = "Palm ready. Press 'C' to capture."

            # --- Draw ROI ---
            cv2.polylines(frame, [pts], isClosed=True, color=(255,0,0), thickness=2)

        # --- Display ---
        cv2.putText(frame, prompt, (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)
        cv2.imshow("Camera Feed", frame)
        if roi_final is not None:
            # show raw ROI as-is (may be large or small depending on crop)
            cv2.imshow("Palm ROI (Raw Crop)", roi_final)

        key = cv2.waitKey(5) & 0xFF
        if key == 27:
            break

        if key == ord('c') and roi_final is not None:
            print("[INFO] Live capture started...")
            required_captures = 10
            capture_count = 0

            while capture_count < required_captures:
                # Grab a fresh frame
                success, frame = cap.read()
                if not success:
                    continue
                frame = cv2.flip(frame, 1)
                clean_frame = frame.copy()
                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                h, w, _ = frame.shape

                # Recalculate the ROI based on current frame
                rotated = cv2.warpAffine(clean_frame, rotation_matrix, (w, h))
                roi_candidate = rotated[int(y_min):int(y_max), int(x_min):int(x_max)]
                roi_to_save = roi_candidate.copy()
                if hand_label == "Left":
                    roi_to_save = cv2.flip(roi_to_save, 1)

                # Save
                outname = os.path.join(OUTPUT_DIR, f"{global_file_index:05d}.jpg")
                cv2.imwrite(outname, roi_to_save)
                global_file_index += 1
                capture_count += 1
                print(f"[INFO] Saved image {capture_count}/{required_captures}")

                # Small delay between frames
                cv2.waitKey(500)

            print("[INFO] Live capture completed.")

            continue


cap.release()
cv2.destroyAllWindows()
